{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c698c5d-d4df-467d-830e-51c0214d2bd8",
   "metadata": {},
   "source": [
    "# Thesis code\n",
    "\n",
    "Notice that there are a lot of repitions within the notebook - this was in order for the different parts to be mostly independant from one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ff2644-f7c3-42b8-9aa2-1d893adee1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All libraries used for our work\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.spatial.distance import cdist\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import explained_variance_score, accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import torch.nn.utils.prune as prune\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "import matplotlib.ticker as ticker\n",
    "import tqdm\n",
    "import copy\n",
    "import optuna\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56667124-0cae-498b-b049-fe16060ae0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seaborn 'Set2' colors - these colors were used for plotting\n",
    "\n",
    "good_colors = ['cornflowerblue', 'darkorange', 'forestgreen', 'hotpink', 'dimgray', 'gold', 'rebeccapurple', 'mediumseagreen']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a3aa4d-74f7-4441-a4f9-4a75bbe524ca",
   "metadata": {},
   "source": [
    "# Clean Noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbac2653-90fb-4cc6-af5d-e9d74a061f93",
   "metadata": {},
   "source": [
    "The following is a noise removal procedure that corresponds to the discussion in section 3.3.1. Inspired by the Boruta algorithm presented by Kursa and Rudnicki in 2010 (https://www.jstatsoft.org/article/view/v036i11) we use Random Forest together with a feature created using Gaussian noise to detect which of features in the data are less important than noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5126e6-31f0-4baf-b5b6-aacb2c5ff178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data prep\n",
    "\n",
    "df = pd.read_csv('Breast_cancer.csv') # Reading the data\n",
    "df['diagnosis'] = df['diagnosis'].apply(lambda x: 1 if x == 'M' else 0) # Encoding the labels as 0s and 1s\n",
    "df = df.drop(['id','Unnamed: 32'], axis = 1) # Removing non-feature columns present in the data\n",
    "df['rand'] = np.random.normal(0,1,len(df.iloc[:,0])) # Adding a noise \"feature\"\n",
    "\n",
    "# Normalizing\n",
    "columns = list(df.columns)\n",
    "columns.remove('diagnosis')\n",
    "\n",
    "for column in columns:\n",
    "    mean = df[column].mean()\n",
    "    std = df[column].std()\n",
    "    df[column] = (df[column]-mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8914ba09-2875-419b-a0f5-633a6f122f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100) # Creatuing a Random Forest model\n",
    "\n",
    "# Calculating importance scores using a Bootstapping approach for the construction of pseudo-confidence intervals\n",
    "n_iterations = 100\n",
    "\n",
    "importance_scores = []\n",
    "for _ in range(n_iterations):\n",
    "    bootstrap_indices = np.random.choice(len(df[columns]), size=len(df[columns]), replace=True)\n",
    "    X_bootstrap = df[columns].iloc[bootstrap_indices]\n",
    "    y_bootstrap = df['diagnosis'].iloc[bootstrap_indices]\n",
    "    \n",
    "    rf.fit(X_bootstrap, y_bootstrap)\n",
    "    importance_scores.append(rf.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43b7460-6b42-4326-9020-6d286a1d3535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the pseudo-confidence intervals for all features\n",
    "HW = []\n",
    "mean_importance = []\n",
    "n = len(importance_scores)\n",
    "for i in range(len(columns)):\n",
    "    temp = []\n",
    "    for j in range(n):\n",
    "        temp.append(importance_scores[j][i])\n",
    "    mean_importance.append(np.mean(temp))\n",
    "    std = np.std(temp, ddof=1)\n",
    "    HW.append(stats.t.ppf(1 - (1 - 0.95) / 2, n-1) * (std / np.sqrt(n)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86380fae-83fe-4cd2-aeed-5dbbf91f4431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Presenting our results\n",
    "plt.figure(figsize=(8,6))\n",
    "x = [i for i in range(len(columns))]\n",
    "plt.bar(x,mean_importance, color = good_colors[0])\n",
    "plt.errorbar(x,mean_importance, yerr=HW, fmt=\".\", color=good_colors[1])\n",
    "plt.xlabel('Features', fontsize = '18')\n",
    "plt.xticks(x, columns)\n",
    "plt.xticks(fontsize = '10', rotation = 90)\n",
    "plt.ylabel('Importance Score', fontsize = '18')\n",
    "plt.yticks(fontsize = '14')\n",
    "plt.grid()\n",
    "plt.savefig('Importance.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc592ea2-5d2d-4952-87dd-5d61eae955a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing a list of features less important than the noise feature that should be removed\n",
    "remove = []\n",
    "threshold = mean_importance[-1] + HW[-1]\n",
    "for i in range(len(columns)-1):\n",
    "    if mean_importance[i] - HW[i] < threshold:\n",
    "        remove.append(columns[i])\n",
    "remove.append(columns[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070b27a2-454c-4e70-ac6b-15a2a7b5b665",
   "metadata": {},
   "source": [
    "In the case of the Wisconsin Breast Cancer data set `remove` was empty, and there was no need for data augmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f251252c-92d5-418f-bbae-ff2d2d6ab32f",
   "metadata": {},
   "source": [
    "# Mutual Information & Harness Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e69a902-7ce0-4cd5-be25-94e995fd39ba",
   "metadata": {},
   "source": [
    "The following is the calculations of the hardness metric discussed in section 3.3.2. The first cell corresponds to the calculation of the mutual information and the secind to the calculation of the hardness metric itself. In both cases the implementation is via a approximation of the distributions using histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d492c76-017e-4d4d-ae09-af298f1ed017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data prep\n",
    "df = pd.read_csv('Breast_cancer.csv') # Reading the data\n",
    "df = df.drop(['id','Unnamed: 32'], axis = 1) # Removing non-feature columns present in the data\n",
    "df['diagnosis'] = df['diagnosis'].apply(lambda x: 1 if x == 'M' else 0) # Encoding the labels as 0s and 1s\n",
    "columns = list(df.columns) # Prepare a list of the features for normalization\n",
    "columns.remove('diagnosis') # Remove the response feature which shouldn't go through normalization\n",
    "\n",
    "# Normalization\n",
    "for column in columns:\n",
    "    mean = df[column].mean()\n",
    "    std = df[column].std()\n",
    "    df[column] = (df[column]-mean)/std\n",
    "    \n",
    "# Preparing the distribution of the labels\n",
    "class_probs = {}\n",
    "total_samples = len(df)\n",
    "for label in set(df['diagnosis']):\n",
    "    class_probs[label] = np.sum(df['diagnosis'] == label) / total_samples\n",
    "    \n",
    "# Set the number of bins to the either 30 or the square root of the number of samples, which ever one's smaller\n",
    "bin_num = min(30,int(np.sqrt(total_samples))) \n",
    "                                              \n",
    "# Preparing the features' distributions\n",
    "feature_probs = {}\n",
    "features = df[columns].values\n",
    "for i in range(features.shape[1]):\n",
    "    # Preparing the 'pure' distribution P(X=x)\n",
    "    feature_values = features[:,i]\n",
    "    bins = np.linspace(min(feature_values),max(feature_values),bin_num+1)\n",
    "    hist = np.histogram(feature_values, bins=bins, density=False)[0]\n",
    "    feature_probs['pure', i] = [hist[i]/sum(hist) for i in range(len(hist))]\n",
    "    # Preparing the dependant distribution P(X=x|Y=y)\n",
    "    for label in set(df['diagnosis']):\n",
    "        mask = (df['diagnosis'] == label)\n",
    "        feature_values = features[mask, i]\n",
    "        hist = np.histogram(feature_values, bins=bins, density=False)[0]\n",
    "        feature_probs[label, i] = [hist[i]/sum(hist) for i in range(len(hist))]\n",
    "\n",
    "# Calculating the mutual information between each feature and the labels\n",
    "ind_mutual_information = []\n",
    "for i in range(features.shape[1]):\n",
    "    temp = 0\n",
    "    for j in range(bin_num):\n",
    "        for label in set(df['diagnosis']):\n",
    "            p_x = feature_probs['pure', i][j]\n",
    "            p_y = class_probs[label]\n",
    "            p_xy = feature_probs[label, i][j]*p_y\n",
    "            if p_xy != 0:\n",
    "                temp = temp + p_xy*np.log(p_xy/(p_x*p_y))\n",
    "    ind_mutual_information.append(temp)\n",
    "\n",
    "# Presenting the sum of mutual information over all features\n",
    "print(\"Mutual Information:\", np.sum(ind_mutual_information))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ddbe2b-6e55-47d0-8e0e-91ef5a732290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data prep\n",
    "df = pd.read_csv('Breast_cancer.csv') # Reading the data\n",
    "df = df.drop(['id','Unnamed: 32'], axis = 1) # Removing non-feature columns present in the data\n",
    "df['diagnosis'] = df['diagnosis'].apply(lambda x: 1 if x == 'M' else 0) # Encoding the labels as 0s and 1s\n",
    "columns = list(df.columns) # Prepare a list of the features for normalization\n",
    "columns.remove('diagnosis') # Remove the response feature which shouldn't go through normalization\n",
    "\n",
    "# Normalization\n",
    "for column in columns:\n",
    "    mean = df[column].mean()\n",
    "    std = df[column].std()\n",
    "    df[column] = (df[column]-mean)/std\n",
    "    \n",
    "# Preparing the distribution of the labels\n",
    "class_probs = {}\n",
    "total_samples = len(df)\n",
    "for label in set(df['diagnosis']):\n",
    "    class_probs[label] = np.sum(df['diagnosis'] == label) / total_samples\n",
    "    \n",
    "# Calculating the entropy of the labels\n",
    "class_entropy = sum([-class_probs[label]*np.log(class_probs[label]) for label in set(df['diagnosis'])])\n",
    "\n",
    "# Set the number of bins to the either 30 or the square root of the number of samples, which ever one's smaller\n",
    "bin_num = min(30,int(np.sqrt(total_samples)))\n",
    "\n",
    "# Preparing the features' distributions\n",
    "feature_probs = {}\n",
    "features = df[columns].values\n",
    "for i in range(features.shape[1]):\n",
    "    # Preparing the 'pure' distribution P(X=x)\n",
    "    feature_values = features[:,i]\n",
    "    bins = np.linspace(min(feature_values),max(feature_values),bin_num+1)\n",
    "    hist = np.histogram(feature_values, bins=bins, density=False)[0]\n",
    "    feature_probs['pure', i] = [hist[i]/sum(hist) for i in range(len(hist))]\n",
    "    # Preparing the dependant distribution P(X=x|Y=y)\n",
    "    for label in set(df['diagnosis']):\n",
    "        mask = (df['diagnosis'] == label)\n",
    "        feature_values = features[mask, i]\n",
    "        hist = np.histogram(feature_values, bins=bins, density=False)[0]\n",
    "        feature_probs[label, i] = [hist[i]/sum(hist) for i in range(len(hist))]\n",
    "\n",
    "# Calculating the mutual information between each feature and the labels\n",
    "ind_mutual_information = []\n",
    "for i in range(features.shape[1]):\n",
    "    temp = 0\n",
    "    for j in range(bin_num):\n",
    "        for label in set(df['diagnosis']):\n",
    "            p_x = feature_probs['pure', i][j]\n",
    "            p_y = class_probs[label]\n",
    "            p_xy = feature_probs[label, i][j]*p_y\n",
    "            if p_xy != 0:\n",
    "                temp = temp + p_xy*np.log(p_xy/(p_x*p_y))\n",
    "    ind_mutual_information.append(temp)\n",
    "\n",
    "# Presenting the resulting hardness metric as it was defined in section 3.3.2.\n",
    "print(\"Hardness metric:\", np.sum(ind_mutual_information)/(len(set(df['diagnosis']))*class_entropy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef1fe4e-24ec-4a7d-8b48-dc042ad494d9",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b9e98d-9d69-4437-9af1-b9f9beb64a76",
   "metadata": {},
   "source": [
    "The following is the implementation of PCA using standard tools from sklearn as one of our measurements of data ID as was described under section 3.3.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7008ae1-ae84-4eaa-8d96-430026113257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data prep\n",
    "df = pd.read_csv('Breast_cancer.csv') # Reading the data\n",
    "df2 = df.drop(['id','diagnosis','Unnamed: 32'], axis = 1) # Removing non-feature columns present in the data as\n",
    "                                                         # well as the labels which are not used for PCA\n",
    "# Normalizing\n",
    "transform = StandardScaler()\n",
    "scaled = transform.fit_transform(df2)\n",
    "scaled_df2 = pd.DataFrame(scaled, index=df2.index, columns=df2.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dae6ed4-2cdc-4c70-ad43-4c51ef6488f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing PCA\n",
    "pca = PCA(n_components=len(df2.columns))\n",
    "components = pca.fit_transform(scaled)\n",
    "\n",
    "# Calculating the cumulative explained variance\n",
    "explained = []\n",
    "for i in range(len(df2.columns)):\n",
    "    explained.append(sum(pca.explained_variance_ratio_[0:i+1]))\n",
    "\n",
    "# Plotting the results    \n",
    "num = [i+1 for i in range(len(df2.columns))]\n",
    "threshold90 = 0.9*np.ones(len(df2.columns))\n",
    "threshold95 = 0.95*np.ones(len(df2.columns))\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.bar(num,explained, color = good_colors[0])\n",
    "plt.plot(num,threshold90,'--', color='red')\n",
    "plt.plot(num,threshold95,'--', color='darkred')\n",
    "plt.xlabel('Number of Components', fontsize = '18')\n",
    "plt.xticks(fontsize = '14')\n",
    "plt.ylabel('Accumulated Explained Variance', fontsize = '18')\n",
    "plt.yticks(fontsize = '14')\n",
    "plt.grid()\n",
    "plt.savefig('PCA.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d07dfa8-0198-43b2-b311-7ec2c4501fd6",
   "metadata": {},
   "source": [
    "# GP+CV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a785325d-059f-46ee-813b-309b7be2b3b4",
   "metadata": {},
   "source": [
    "The following is the implementation of the GP+CV algorithm as was described under section 3.3.3 as a second method of data ID estimation based on the papers by Grassberger and Procaccia from 1983 (https://www.sciencedirect.com/science/article/abs/pii/0167278983902981) and by Camastra and Vinciarelli from 2002 (https://ieeexplore.ieee.org/document/1039212).\n",
    "\n",
    "Notice that below we use `scaled` which was created in the PCA part above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd83b2e-cac6-4158-a686-990f54a51a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GP(array):\n",
    "    # A function that creates the data required for the curve to calculate the\n",
    "    # correlation dimension using the GP algorithm - i.e., it calculates the\n",
    "    # natural logarithm of the correlation integral for various values of r.\n",
    "    # 'array' is a numpy array that has samples as rows and columns as features\n",
    "    # (the usual way...).\n",
    "    dist_matrix = cdist(array, array, metric='euclidean') # Calculating a distance matrix for the data\n",
    "    dist_matrix1 = dist_matrix.copy() # Creating a second matrix with 'inf' values on the diagonal to eliminate zeros\n",
    "    dist_matrix1[np.diag_indices_from(dist_matrix1)] = np.inf\n",
    "    N = array.shape[0]\n",
    "    \n",
    "    # Creating a distances vector that covers the relevant distances (those that exist between data points)\n",
    "    if np.min(dist_matrix1) == 0:\n",
    "        r = np.logspace(-5,np.log10(np.max(dist_matrix)),1000)\n",
    "    else:\n",
    "        r = np.logspace(np.log10(np.min(dist_matrix1)),np.log10(np.max(dist_matrix)),1000)\n",
    "    \n",
    "    # Calculating the correlation integral\n",
    "    logC = []\n",
    "    for i in range(len(r)):\n",
    "        logC.append(np.log((sum(sum(dist_matrix <= r[i]))-N)/(N*(N-1))))\n",
    "    \n",
    "    # Saving the calculating result ignoring senless data that's coming from empty distance bins\n",
    "    infnum = sum(np.isinf(logC))\n",
    "    logC = np.array(logC)\n",
    "    r[0:infnum] = r[infnum]\n",
    "    logC[0:infnum] = logC[infnum]\n",
    "    return [np.log(r),logC]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ec198f-1b0e-48f7-8101-1a24c83b3617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating all required data for the GP+CV algorithm reference curve using random values in hypercubes\n",
    "\n",
    "# A dataframe to include all of the relevant data\n",
    "df = dict()\n",
    "\n",
    "# Performing with the GP algorithm on random hypercube data\n",
    "for i in range(scaled.shape[1]):\n",
    "    res = GP(np.random.rand(scaled.shape[0],i+1))\n",
    "    df['logr ' + str(i+1)] = res[0]\n",
    "    df['logC ' + str(i+1)] = res[1]\n",
    "\n",
    "# Performing with the GP algorithm on the data of interest\n",
    "res = GP(scaled)\n",
    "df['logr data'] = res[0]\n",
    "df['logC data'] = res[1]\n",
    "\n",
    "# Saving all the data to be further analyzed using MATLAB\n",
    "df = pd.DataFrame.from_dict(df)\n",
    "df.to_csv('GPCV.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d16ba6c-01ff-4b5c-bc85-9a7e71ae7094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the results of our MATLAB analysis\n",
    "\n",
    "# Main graph\n",
    "logr = df['logr data']\n",
    "logC = df['logC data']\n",
    "x = np.linspace(np.min(logr),1.4,1000)\n",
    "y = 7.095*x - 10.36\n",
    "\n",
    "# Reference curve\n",
    "ref_d = [1,2,3,4,5,6,7,8,9,10]\n",
    "ref_D = [0.9833,1.974,2.897,3.683,4.575,5.22,5.659,6.43,6.987,7.893]\n",
    "\n",
    "# Plotting the main graph\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(logr, logC, '.', color=good_colors[0])\n",
    "plt.plot(x,y,'--',color=good_colors[1])\n",
    "plt.xlabel(r'$\\log{\\left(r\\right)}$',size='18')\n",
    "plt.ylabel(r'$\\log{\\left(C_{m}\\left(r\\right)\\right)}$',size='18')\n",
    "plt.xticks(fontsize='14')\n",
    "plt.yticks(fontsize='14')\n",
    "\n",
    "# Plotting the reference curve in a desired location\n",
    "ax_inset = plt.axes([0.6, 0.2, 0.25, 0.25])  # [left, bottom, width, height]\n",
    "ax_inset.plot(ref_d, ref_D, 'o-', color=good_colors[2], label='Reference Curve')\n",
    "ax_inset.set_xlabel('d',size='14')\n",
    "ax_inset.set_ylabel('D',size='14')\n",
    "\n",
    "# Saving and presenting figure\n",
    "plt.savefig('GPCV.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfb7b97-785c-4f33-b33f-003b88267ae4",
   "metadata": {},
   "source": [
    "# Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8723cafc-8edb-41b1-9f00-11a9ec3b4ee8",
   "metadata": {},
   "source": [
    "Implementation of autoencoders as a third method of data ID estimation which was described under section 3.3.3. The first part was used for hyperparameter tuning of both the single hidden layer and three hidden layer autoencoders. The following parts present the results for the two different autoencoders using values obtained through the optimization process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4446ea-1659-4c63-86a4-f8e2fb42c192",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b7b4ee-fdaf-4448-8172-ee116fc2408b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a class that assists with feeding data into PyTorch\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, features):\n",
    "        self.features = features\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        feature = self.features[index]\n",
    "        return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00c5219-c7c9-4f41-abbf-65ef75215d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for training\n",
    "def train(dataloader, model, loss_fn, optimizer,device):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.train()\n",
    "    train_loss, correct = 0, 0\n",
    "    for batch, X in enumerate(dataloader):\n",
    "        X = X.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, X)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6655043-9eae-471b-abf6-e84aee773a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for testing\n",
    "def test(dataloader, model, loss_fn, device):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X in dataloader:\n",
    "            X = X.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, X).item()\n",
    "    test_loss /= num_batches\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdae3f3-801d-401c-b2f4-ed5362c885eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A PyTorch based class for a three hidden layer autoencoder with no encoding to and decoding from a lower dimension\n",
    "class AE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Structure and activation functions\n",
    "        self.hidden1 = nn.Linear(30, 30)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.hidden2 = nn.Linear(30, 30)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.hidden3 = nn.Linear(30, 30)\n",
    "        self.act3 = nn.ReLU()\n",
    "        self.output = nn.Linear(30, 30)\n",
    "        self.act_output = nn.Identity()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.act1(self.hidden1(x))\n",
    "        x = self.act2(self.hidden2(x))\n",
    "        x = self.act3(self.hidden3(x))\n",
    "        x = self.act_output(self.output(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c4ae41-4555-4a3e-a8d5-d1a0ce30fee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A PyTorch based class for a single hidden layer autoencoder with no encoding to and decoding from a lower dimension\n",
    "class AE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Structure and activation functions\n",
    "        self.hidden = nn.Linear(30, 30)\n",
    "        self.act = nn.ReLU()\n",
    "        self.output = nn.Linear(30, 30)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.act(self.hidden(x))\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcd1ac1-ae61-4534-bb1c-1a8c8000bf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for training over 200 epochs with an early-stopping condition over the validation loss\n",
    "def train_and_evaluate(model, learning_rate, batch_size, train_dataset, val_dataset, device):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=len(val_dataset))\n",
    "    best_loss = + np.inf   # init to positive infinity\n",
    "    loss_fn = nn.MSELoss()\n",
    "    for epoch in range(200):\n",
    "        train(train_loader, model, loss_fn, optimizer, device)\n",
    "        test_loss = test(val_loader, model, loss_fn, device)\n",
    "        if test_loss < best_loss:\n",
    "              best_loss = test_loss\n",
    "\n",
    "    return best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c41cc59-04d3-498e-b980-02f52d10ec66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read data into objects that PyTorch can handle - it is required to be in function form for\n",
    "# compatibility with the optuna library\n",
    "def load_data():\n",
    "    # Reading\n",
    "    df3 = pd.read_csv('Breast_cancer.csv')\n",
    "    df4 = df3.drop(['id','diagnosis','Unnamed: 32'], axis = 1)\n",
    "\n",
    "    # Splitting test and validation\n",
    "    X_train,X_val = train_test_split(df4,test_size=0.3,random_state=42)\n",
    "\n",
    "    # Normalizing\n",
    "    for column in df4.columns:\n",
    "        mean = X_train[column].mean()\n",
    "        std = X_train[column].std()\n",
    "        X_train[column] = (X_train[column]-mean)/std\n",
    "        X_val[column] = (X_val[column]-mean)/std\n",
    "\n",
    "    # Transforming data to a Pytorch form\n",
    "    X_train = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "    X_val = torch.tensor(X_val.values, dtype=torch.float32)\n",
    "\n",
    "    train_dataset = CustomDataset(X_train)\n",
    "    val_dataset = CustomDataset(X_val)\n",
    "    return train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdec7042-8f6b-46fb-8208-a9148458152c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the objective function used by optuna to optimize the hyperparameters\n",
    "def objective(trial):\n",
    "    train_data, val_data = load_data()\n",
    "    \n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n",
    "    batch_size = trial.suggest_int('batch_size', 4, 64, log=True)\n",
    "    # batch_size = trial.suggest_categorical('batch_size', [1, 2, 3, 4])\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = AE().to(device)\n",
    "    loss = train_and_evaluate(model, learning_rate, batch_size, train_data, val_data, device)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd10b41-b36d-43bb-908c-198306f8d7c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Performing optimization using optuna\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "best_params = study.best_params\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a13658-f102-475a-8d4f-7651bc2e0a3b",
   "metadata": {},
   "source": [
    "## ID Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e95cb9-e92b-442f-a51d-3ba5c479c6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a class that assists with feeding data into PyTorch\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, features):\n",
    "        self.features = features\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        feature = self.features[index]\n",
    "        return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e17dad-d09a-4db4-ac8b-bfead4630e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data prep\n",
    "\n",
    "# Reading\n",
    "df3 = pd.read_csv('Breast_cancer.csv')\n",
    "df4 = df3.drop(['id','diagnosis','Unnamed: 32'], axis = 1)\n",
    "\n",
    "# Splitting test and validation\n",
    "X_train,X_val = train_test_split(df4,test_size=0.3,random_state=42)\n",
    "\n",
    "# Normalizing\n",
    "for column in df4.columns:\n",
    "    mean = X_train[column].mean()\n",
    "    std = X_train[column].std()\n",
    "    X_train[column] = (X_train[column]-mean)/std\n",
    "    X_val[column] = (X_val[column]-mean)/std\n",
    "\n",
    "# Transforming data to a Pytorch form\n",
    "X_train = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "X_val = torch.tensor(X_val.values, dtype=torch.float32)\n",
    "\n",
    "# batch_size = 4 # For autoencoder-3\n",
    "batch_size = 3 # For autoencoder-1\n",
    "train_dataset = CustomDataset(X_train)\n",
    "val_dataset = CustomDataset(X_val)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480d6cbb-3039-42b0-ba21-f0dc9b3db9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for training\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.train()\n",
    "    train_loss, correct = 0, 0\n",
    "    for batch, X in enumerate(dataloader):\n",
    "        X = X.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, X)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3931ea-09bf-4243-84de-03690128f95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for testing\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, EV = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X in dataloader:\n",
    "            X = X.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, X).item()\n",
    "            EV += explained_variance_score(pred.cpu(), X.cpu()) # Explained variance calculation\n",
    "    test_loss /= num_batches\n",
    "    EV /= num_batches\n",
    "    return test_loss, EV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018c0016-2f51-4e9e-bfd8-939d9747b7be",
   "metadata": {},
   "source": [
    "### Autoencoder-3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1dc6e5a-43e8-48d6-9fb6-f41cafb43142",
   "metadata": {},
   "source": [
    "Estimating ID using a three hidden layer autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaebd33a-301c-40b3-96ab-eea7fcf2b3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A PyTorch based class for a three hidden layer autoencoder, with `hid1` and `hid2` controlling the level of\n",
    "# encoding and decoding\n",
    "class AE(nn.Module):\n",
    "    def __init__(self,hid1,hid2):\n",
    "        super().__init__()\n",
    "        # Structure and activation functions\n",
    "        self.hidden1 = nn.Linear(30, hid1)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.hidden2 = nn.Linear(hid1, hid2)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.hidden3 = nn.Linear(hid2, hid1)\n",
    "        self.act3 = nn.ReLU()\n",
    "        self.output = nn.Linear(hid1, 30)\n",
    "        self.act_output = nn.Identity()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.act1(self.hidden1(x))\n",
    "        x = self.act2(self.hidden2(x))\n",
    "        x = self.act3(self.hidden3(x))\n",
    "        x = self.act_output(self.output(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c73204-1eb6-4299-9596-e6bd1905d349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the explained variance as a function of the narrowest layer dimension\n",
    "\n",
    "EV = [] # A list to hold explained variance values\n",
    "dim = [] # A list to hold the narrowest layer dimensions\n",
    "learning_rate = 0.0014283641459297775\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(torch.cuda.get_device_name(device))\n",
    "n_epochs = 200\n",
    "\n",
    "\n",
    "is_not_done = 1\n",
    "hid1 = 30 # Starting with no encoding to or decoding from a lower dimension\n",
    "hid2 = 30\n",
    "cond1 = 1\n",
    "\n",
    "while is_not_done:\n",
    "    # Training an Autoencoder and storing the explained variance achieved\n",
    "    model = AE(hid1,hid2).to(device)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    best_loss = + np.inf\n",
    "    best_EV = + np.inf\n",
    "    for epoch in range(n_epochs):\n",
    "        train(train_loader, model, loss_fn, optimizer)\n",
    "        epoch_loss, EV_temp = test(val_loader, model, loss_fn)\n",
    "        if epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            best_EV = EV_temp\n",
    "    dim_temp = hid2\n",
    "    print('EV = ' + str(best_EV) + ', dim = ' + str(hid2))\n",
    "\n",
    "    # When we cross the 0.95 threshold, we check if hid2+1 allows more than 0.95 explained variance\n",
    "    if best_EV < 0.95 and cond1:\n",
    "        cond1 = 0\n",
    "        model = AE(hid1,hid2+1).to(device)\n",
    "        loss_fn = nn.MSELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        best_loss = + np.inf\n",
    "        new_EV = + np.inf\n",
    "        for epoch in range(n_epochs):\n",
    "            train(train_loader, model, loss_fn, optimizer)\n",
    "            epoch_loss, EV_temp = test(val_loader, model, loss_fn)\n",
    "            if epoch_loss < best_loss:\n",
    "                best_loss = epoch_loss\n",
    "                new_EV = EV_temp\n",
    "        EV.append(new_EV)\n",
    "        dim.append(hid2+1)\n",
    "        print('EV = ' + str(EV[-1]) + ', dim = ' + str(dim[-1]))\n",
    "\n",
    "    # When we cross the 0.90 threshold, we check if hid2+1 allows more than 0.9 explained variance\n",
    "    if best_EV < 0.90:\n",
    "        is_not_done = 0 # We terminate the while loop after this one last calculation\n",
    "        model = AE(hid1,hid2+1).to(device)\n",
    "        loss_fn = nn.MSELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        best_loss = + np.inf\n",
    "        new_EV = + np.inf\n",
    "        for epoch in range(n_epochs):\n",
    "            train(train_loader, model, loss_fn, optimizer)\n",
    "            epoch_loss, EV_temp = test(val_loader, model, loss_fn)\n",
    "            if epoch_loss < best_loss:\n",
    "                best_loss = epoch_loss\n",
    "                new_EV = EV_temp\n",
    "        EV.append(new_EV)\n",
    "        dim.append(hid2+1)\n",
    "        print('EV = ' + str(EV[-1]) + ', dim = ' + str(dim[-1]))\n",
    "    \n",
    "    # Saving the values from *before* the if loops so we get the values in the correct order \n",
    "    EV.append(best_EV)\n",
    "    dim.append(dim_temp)\n",
    "    \n",
    "    # Lowering the dimension\n",
    "    hid1 = hid1 - 1\n",
    "    hid2 = hid2 - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c3425f-031e-4744-bf4e-21ef7748c5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the results\n",
    "dict1 = dict()\n",
    "dict1['EV'] = EV\n",
    "dict1['dim'] = dim\n",
    "df = pd.DataFrame.from_dict(dict1)\n",
    "df.to_csv('Autoencoder-3.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e2c8b7-21b5-44ac-9e00-0d1f5e084415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the results\n",
    "\n",
    "df = pd.read_csv('Autoencoder-3.csv') \n",
    "\n",
    "x = [min(df['dim']), 30]\n",
    "threshold90 = [0.90,0.90]\n",
    "threshold95 = [0.95,0.95]\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(df['dim'],df['EV'], 'o-', color = good_colors[0])\n",
    "plt.plot(x,threshold90,'--', color='red')\n",
    "plt.plot(x,threshold95,'--', color='darkred')\n",
    "plt.xlabel('Narrow Layer\\'s Width', fontsize = '18')\n",
    "plt.xticks(fontsize = '14')\n",
    "plt.ylabel('Explained Variance', fontsize = '18')\n",
    "plt.yticks(fontsize = '14')\n",
    "plt.grid()\n",
    "plt.savefig('AE-3.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f903e8-a82c-46e7-a09f-54fb3e54fe12",
   "metadata": {},
   "source": [
    "### Autoencoder-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832e1a71-0164-4d09-8aa2-64f3accb2901",
   "metadata": {},
   "source": [
    "Estimating ID using a single hidden layer autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda54a87-e46b-418c-a7fe-1566d8f7483d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A PyTorch based class for a single hidden layer autoencoder, with `hid` controlling the level of encoding and\n",
    "# decoding\n",
    "class AE(nn.Module):\n",
    "    def __init__(self,hid):\n",
    "        super().__init__()\n",
    "        # Structure and activation functions\n",
    "        self.hidden = nn.Linear(30, hid)\n",
    "        self.act = nn.ReLU()\n",
    "        self.output = nn.Linear(hid, 30)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.act(self.hidden(x))\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5233a0ed-6b29-4fde-8060-387e20515925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the explained variance as a function of the narrowest layer dimension\n",
    "\n",
    "EV = [] # A list to hold explained variance values\n",
    "dim = [] # A list to hold the narrowest layer dimensions\n",
    "learning_rate = 0.0020056404664158065\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(torch.cuda.get_device_name(device))\n",
    "n_epochs = 200\n",
    "\n",
    "\n",
    "is_not_done = 1 # Starting with no encoding to or decoding from a lower dimension\n",
    "hid = 30\n",
    "\n",
    "while is_not_done:\n",
    "    # Training an Autoencoder and storing the explained variance achieved\n",
    "    model = AE(hid).to(device)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    best_loss = + np.inf\n",
    "    best_EV = + np.inf\n",
    "    for epoch in range(n_epochs):\n",
    "        train(train_loader, model, loss_fn, optimizer)\n",
    "        epoch_loss, EV_temp = test(val_loader, model, loss_fn)\n",
    "        if epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            best_EV = EV_temp\n",
    "    dim_temp = hid\n",
    "    print('EV = ' + str(best_EV) + ', dim = ' + str(hid))\n",
    "\n",
    "    # When we cross the 0.90 threshold, we stop the calculation\n",
    "    if best_EV < 0.90:\n",
    "        is_not_done = 0\n",
    "        \n",
    "    EV.append(best_EV)\n",
    "    dim.append(dim_temp)\n",
    "    # Lowering the dimension\n",
    "    hid = hid - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a95a8ab-13a4-4b7f-9547-8e131beb4e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the results\n",
    "dict1 = dict()\n",
    "dict1['EV'] = EV\n",
    "dict1['dim'] = dim\n",
    "df = pd.DataFrame.from_dict(dict1)\n",
    "df.to_csv('Autoencoder-1.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7af7a4-51b1-4ec3-8126-a6b55b0f4a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the results\n",
    "\n",
    "df = pd.read_csv('Autoencoder-1.csv') \n",
    "\n",
    "x = [min(df['dim']), 30]\n",
    "threshold90 = [0.90,0.90]\n",
    "threshold95 = [0.95,0.95]\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(df['dim'],df['EV'], 'o-', color = good_colors[0])\n",
    "plt.plot(x,threshold90,'--', color='red')\n",
    "plt.plot(x,threshold95,'--', color='darkred')\n",
    "plt.xlabel('Narrow Layer\\'s Width', fontsize = '18')\n",
    "plt.xticks(fontsize = '14')\n",
    "plt.ylabel('Explained Variance', fontsize = '18')\n",
    "plt.yticks(fontsize = '14')\n",
    "plt.grid()\n",
    "plt.savefig('AE-1.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4964b05c-9399-4343-a3fe-1ffd62957cf6",
   "metadata": {},
   "source": [
    "# Li et al. Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cde4654-9c4f-46d0-a3a1-a7c6165ea367",
   "metadata": {},
   "source": [
    "In the following we present the implementation of the Li et al. method used for neural network ID estimation as was described in section 3.3.4 based on the original work by Li et al. from 2018 (https://arxiv.org/abs/1804.08838). The first part deals with hyperparameter tuning - including the dimensions of a fully connected network architecture that would be used as the basis for the employment of the Li et al. method. The following part deals with the ID estimation itself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e509db-8170-42a4-90ba-b7d58bcfbf5d",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4b0b10-5eaa-41b1-8845-96c3654c298c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a class that assists with feeding data into PyTorch\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        feature = self.features[index]\n",
    "        label = self.labels[index]\n",
    "        return feature, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20fbc9e-6b4f-4795-af0f-2b8914585fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for training\n",
    "def train(dataloader, model, loss_fn, optimizer, device):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.train()\n",
    "    train_loss, correct = 0, 0\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y.view(-1, 1))\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ee0477-3a84-4406-a3b2-4bf7f7301449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for testing\n",
    "def test(dataloader, model, loss_fn,device):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred, y.view(-1, 1))\n",
    "            pred = pred.round()\n",
    "            \n",
    "            correct += (pred == y.view(-1, 1)).sum().item()\n",
    "            test_loss += loss.item()\n",
    "    correct /= size\n",
    "    test_loss /= num_batches\n",
    "    return correct, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d59dc24-cea4-4b3d-af60-1639d9e868c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A PyTorch based class for a two hidden layer fully connected architecture. We do not use the \"regular\" means\n",
    "# of creating a PyTorch model in order to be uniform with the manner in which we later define the class of \n",
    "# \"Li models\" where we found that this \"manual\" model construction is required for the sub-space training \n",
    "# instead of training using all model parameters\n",
    "class RegModel(nn.Module):\n",
    "    def __init__(self, hid1, hid2):\n",
    "        super(RegModel, self).__init__()\n",
    "        self.weight1 = nn.Parameter(torch.empty(hid1, 30), requires_grad = True)\n",
    "        nn.init.xavier_uniform_(self.weight1)\n",
    "        self.bias1 = nn.Parameter(torch.randn(hid1), requires_grad = True)\n",
    "        self.weight2 = nn.Parameter(torch.empty(hid2, hid1), requires_grad = True)\n",
    "        nn.init.xavier_uniform_(self.weight2)\n",
    "        self.bias2 = nn.Parameter(torch.randn(hid2), requires_grad = True)\n",
    "        self.weight3 = nn.Parameter(torch.empty(1, hid2), requires_grad = True)\n",
    "        nn.init.xavier_uniform_(self.weight3)\n",
    "        self.bias3 = nn.Parameter(torch.randn(1), requires_grad = True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(torch.matmul(x, self.weight1.t()) + self.bias1)\n",
    "        x = torch.relu(torch.matmul(x, self.weight2.t()) + self.bias2)\n",
    "        x = torch.sigmoid(torch.matmul(x, self.weight3.t()) + self.bias3)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3717017b-179b-408b-bcfd-2eb812452445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for 200 epoch training and evaluation with an early stopping condition of minimum validation loss\n",
    "def train_and_evaluate(model, learning_rate, batch_size, train_dataset, val_dataset, device):\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=len(val_dataset))\n",
    "    best_acc = - np.inf   # init to negative infinity\n",
    "    best_loss = + np.inf # init to positive infinity\n",
    "    loss_fn = nn.BCELoss()\n",
    "    for epoch in range(200):\n",
    "        train(train_loader, model, loss_fn, optimizer, device)\n",
    "        test_acc, loss = test(val_loader, model, loss_fn, device)\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            best_acc = test_acc\n",
    "    print('Accuracy: ' + str(100*test_acc) + '%')\n",
    "    return best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff2dcaf-e4be-42bb-b9fb-5468694da18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read data into objects that PyTorch can handle - it is required to be in function form for\n",
    "# compatibility with the optuna library\n",
    "def load_data():\n",
    "    # Reading\n",
    "    df3 = pd.read_csv('Breast_cancer.csv')\n",
    "    df4 = df3.drop(['id','Unnamed: 32'], axis = 1)\n",
    "    columns = list(df4.columns)\n",
    "    columns.remove('diagnosis')\n",
    "\n",
    "    # Encoding the labels\n",
    "    class_to_idx = {class_label: idx for idx, class_label in enumerate(df4['diagnosis'].unique())}\n",
    "    df4['label_idx'] = df4['diagnosis'].map(class_to_idx)\n",
    "\n",
    "    # Splitting test and validation\n",
    "    X_train, X_val, y_train, y_val = train_test_split(df4[columns],df4['label_idx'],test_size=0.3,random_state=42)\n",
    "\n",
    "    # Normalizing\n",
    "    for column in columns:\n",
    "        mean = X_train[column].mean()\n",
    "        std = X_train[column].std()\n",
    "        X_train[column] = (X_train[column]-mean)/std\n",
    "        X_val[column] = (X_val[column]-mean)/std\n",
    "\n",
    "    # Transforming data to a Pytorch form\n",
    "    X_train = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "    X_val = torch.tensor(X_val.values, dtype=torch.float32)\n",
    "    y_train = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "    y_val = torch.tensor(y_val.values, dtype=torch.float32)\n",
    "\n",
    "    train_dataset = CustomDataset(X_train, y_train)\n",
    "    val_dataset = CustomDataset(X_val, y_val)\n",
    "    return train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cab0a8-e3fa-420a-9f2e-a546eba01add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the objective function used by optuna to optimize the hyperparameters\n",
    "def objective(trial):\n",
    "    train_data, val_data = load_data()\n",
    "    \n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-1, log=True)\n",
    "    batch_size = trial.suggest_int('batch_size', 4, 64, log=True)\n",
    "    hid1 = trial.suggest_int('hid1', 10, 100, log=True)\n",
    "    hid2 = trial.suggest_int('hid2', 10, 100, log=True)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = RegModel(hid1,hid2).to(device)\n",
    "    loss = train_and_evaluate(model, learning_rate, batch_size, train_data, val_data, device)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69fef36-234b-4d87-a2ac-71228d983413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing optimization using optuna\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "best_params = study.best_params\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5b7f07-1c88-4bbf-a660-668290d39206",
   "metadata": {},
   "source": [
    "## ID Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544a073c-afd0-4a9e-bd78-db79b4be549f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a class that assists with feeding data into PyTorch\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        feature = self.features[index]\n",
    "        label = self.labels[index]\n",
    "        return feature, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c282ec-ebee-451d-9af9-66cc0b54583e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data prep\n",
    "\n",
    "# Reading\n",
    "df3 = pd.read_csv('Breast_cancer.csv')\n",
    "df4 = df3.drop(['id','Unnamed: 32'], axis = 1)\n",
    "columns = list(df4.columns)\n",
    "columns.remove('diagnosis')\n",
    "\n",
    "# Encoding the labels\n",
    "class_to_idx = {class_label: idx for idx, class_label in enumerate(df4['diagnosis'].unique())}\n",
    "df4['label_idx'] = df4['diagnosis'].map(class_to_idx)\n",
    "\n",
    "# Splitting test and validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(df4[columns],df4['label_idx'],test_size=0.3,random_state=42)\n",
    "\n",
    "# Normalizing\n",
    "for column in columns:\n",
    "    mean = X_train[column].mean()\n",
    "    std = X_train[column].std()\n",
    "    X_train[column] = (X_train[column]-mean)/std\n",
    "    X_val[column] = (X_val[column]-mean)/std\n",
    "\n",
    "# Transforming data to a Pytorch form\n",
    "X_train = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "X_val = torch.tensor(X_val.values, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val.values, dtype=torch.float32)\n",
    "\n",
    "batch_size = 7\n",
    "train_dataset = CustomDataset(X_train, y_train)\n",
    "val_dataset = CustomDataset(X_val, y_val)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adef64e-94e5-47fa-a924-ca507321f846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for training\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y.view(-1, 1))\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a900910-8bc4-4b72-8610-c7271a435869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for testing\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred, y.view(-1, 1))\n",
    "            pred = pred.round()\n",
    "            \n",
    "            correct += (pred == y.view(-1, 1)).sum().item()\n",
    "            test_loss += loss.item()\n",
    "    correct /= size\n",
    "    test_loss /= num_batches\n",
    "    return correct, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d6d65c-6336-42de-b985-ec70da6a8d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for 200 epoch training and evaluation with an early stopping condition of minimum validation loss\n",
    "def train_and_evaluate(model, learning_rate, train_loader, val_loader):\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    best_acc = - np.inf   # init to negative infinity\n",
    "    best_loss = + np.inf # init to positive infinity\n",
    "    loss_fn = nn.BCELoss()\n",
    "    for epoch in range(200):\n",
    "        train(train_loader, model, loss_fn, optimizer)\n",
    "        test_acc, loss = test(val_loader, model, loss_fn)\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            best_acc = test_acc\n",
    "    return best_acc, best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ca5fe6-627e-43d3-aa82-20c3d47fb4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A version of the previous function that matches a Li model - a model where the optimizer is only accessible \n",
    "# to the vector representing the sub-space\n",
    "def train_and_evaluate_Li(model, learning_rate, train_loader, val_loader):\n",
    "    optimizer = optim.SGD([model.u], lr=learning_rate)\n",
    "    best_acc = - np.inf   # init to negative infinity\n",
    "    best_loss = + np.inf # init to positive infinity\n",
    "    loss_fn = nn.BCELoss()\n",
    "    for epoch in range(200):\n",
    "        train(train_loader, model, loss_fn, optimizer)\n",
    "        test_acc, loss = test(val_loader, model, loss_fn)\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            best_acc = test_acc\n",
    "    return best_acc, best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce95382d-9375-4370-84c5-f83989d594f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A PyTorch based class for a two hidden layer fully connected architecture - same as the hyperparameter tuning part\n",
    "class RegModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RegModel, self).__init__()\n",
    "        self.weight1 = nn.Parameter(torch.empty(71, 30), requires_grad = True)\n",
    "        nn.init.xavier_uniform_(self.weight1)\n",
    "        self.bias1 = nn.Parameter(torch.randn(71), requires_grad = True)\n",
    "        self.weight2 = nn.Parameter(torch.empty(30, 71), requires_grad = True)\n",
    "        nn.init.xavier_uniform_(self.weight2)\n",
    "        self.bias2 = nn.Parameter(torch.randn(30), requires_grad = True)\n",
    "        self.weight3 = nn.Parameter(torch.empty(1, 30), requires_grad = True)\n",
    "        nn.init.xavier_uniform_(self.weight3)\n",
    "        self.bias3 = nn.Parameter(torch.randn(1), requires_grad = True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(torch.matmul(x, self.weight1.t()) + self.bias1)\n",
    "        x = torch.relu(torch.matmul(x, self.weight2.t()) + self.bias2)\n",
    "        x = torch.sigmoid(torch.matmul(x, self.weight3.t()) + self.bias3)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bf0d53-8df8-4ba5-8f5d-c2dc9604029c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A PyTorch based class for a Li model - two hidden layer fully connected architecture but with additional \n",
    "# parameters: one that defines the sub-space where training takes place and 6 projection matrices that project\n",
    "# the changes in the sub-space to the full architecture\n",
    "class LiModel(nn.Module):\n",
    "    def __init__(self, reduced_dim):\n",
    "        super(LiModel, self).__init__()\n",
    "        \n",
    "        # The sub-space is initialized as a vector of zeros - it is important to do so since the sub-space\n",
    "        # actually stands for *changes* from the network's initialization values and does not stand for the \n",
    "        # weights themselves\n",
    "        self.reduced_dim = reduced_dim\n",
    "        self.u = nn.Parameter(torch.zeros(reduced_dim), requires_grad = True)\n",
    "        \n",
    "        # The \"regular\" network\n",
    "        self.weight1 = nn.Parameter(torch.empty(71, 30), requires_grad = False)\n",
    "        nn.init.xavier_uniform_(self.weight1)\n",
    "        self.bias1 = nn.Parameter(torch.randn(71), requires_grad = False)\n",
    "        self.weight2 = nn.Parameter(torch.empty(30, 71), requires_grad = False)\n",
    "        nn.init.xavier_uniform_(self.weight2)\n",
    "        self.bias2 = nn.Parameter(torch.randn(30), requires_grad = False)\n",
    "        self.weight3 = nn.Parameter(torch.empty(1, 30), requires_grad = False)\n",
    "        nn.init.xavier_uniform_(self.weight3)\n",
    "        self.bias3 = nn.Parameter(torch.randn(1), requires_grad = False)\n",
    "        \n",
    "        # Projection matrices from the sub-space to the full network\n",
    "        self.P1 = nn.Parameter(torch.nn.init.orthogonal_(torch.empty(71*30, reduced_dim)), requires_grad = False)\n",
    "        self.P1B = nn.Parameter(torch.nn.init.orthogonal_(torch.empty(71, reduced_dim)), requires_grad = False)\n",
    "        self.P2 = nn.Parameter(torch.nn.init.orthogonal_(torch.empty(30*71, reduced_dim)), requires_grad = False)\n",
    "        self.P2B = nn.Parameter(torch.nn.init.orthogonal_(torch.empty(30, reduced_dim)), requires_grad = False)\n",
    "        self.P3 = nn.Parameter(torch.nn.init.orthogonal_(torch.empty(30, reduced_dim)), requires_grad = False)\n",
    "        self.P3B = nn.Parameter(torch.nn.init.orthogonal_(torch.empty(1, reduced_dim)), requires_grad = False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(torch.matmul(x, self.weight1.t() + torch.matmul(self.u, self.P1.t()).view((30,-1))) + self.bias1 + torch.matmul(self.u, self.P1B.t()))\n",
    "        x = torch.relu(torch.matmul(x, self.weight2.t() + torch.matmul(self.u, self.P2.t()).view((71,-1))) + self.bias2 + torch.matmul(self.u, self.P2B.t()))\n",
    "        x = torch.sigmoid(torch.matmul(x, self.weight3.t() + torch.matmul(self.u, self.P3.t()).view((30,-1))) + self.bias3 + torch.matmul(self.u, self.P3B.t()))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f13a85-484d-43d0-98f2-5a7d6a5cc234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating results\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(torch.cuda.get_device_name(device))\n",
    "\n",
    "learning_rate = 0.006882672576552301\n",
    "replicas = 30 # We use 30 replicas to have statistics of the results\n",
    "\n",
    "# Training a \"regular\" network for defining a threshold for the Li et al. method\n",
    "res = []\n",
    "temp = [31*71 + 72*30 + 31]\n",
    "for i in range(replicas):\n",
    "    model = RegModel().to(device)\n",
    "    acc, loss = train_and_evaluate(model, learning_rate, train_loader, val_loader)\n",
    "    temp.append(acc)\n",
    "    print(\"Replica \" + str(i+1) + \" of full model. Avg accuracy so far: \" + str(100*sum(temp[1:])/len(temp[1:])) + \"%.\")\n",
    "res.append(temp)\n",
    "\n",
    "# Calculating the thresholds of ID definition and for stopping the computation\n",
    "threshold = 0.9*sum(temp[1:])/len(temp[1:])\n",
    "stopping = 0.95*sum(temp[1:])/len(temp[1:])\n",
    "\n",
    "# Calculating the performance using the Li et al. method, incrementally enlarging the sub-space dimension\n",
    "is_not_done = 1\n",
    "current_parameters = 1\n",
    "while is_not_done:\n",
    "    temp = [current_parameters]\n",
    "    for i in range(replicas):\n",
    "        model = LiModel(current_parameters).to(device)\n",
    "        acc, loss = train_and_evaluate_Li(model, learning_rate, train_loader, val_loader)\n",
    "        temp.append(acc)\n",
    "        print(\"Replica \" + str(i+1) + \" of a \" + str(current_parameters) + \" parameter model. Avg accuracy so far: \" + str(100*sum(temp[1:])/len(temp[1:])) + \"%.\")\n",
    "    res.append(temp)\n",
    "    \n",
    "    current_parameters = current_parameters + 1\n",
    "    current_acc = sum(temp[1:])/len(temp[1:])\n",
    "    if current_acc > stopping:\n",
    "        is_not_done = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e0d856-42a7-4fb5-9458-00e0e3dc78c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the results\n",
    "df_Li = pd.DataFrame(np.array(res))\n",
    "df_Li.to_csv(\"LiEtAl.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e0882e-3a0f-40e2-9433-149b6ccf1521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the results\n",
    "\n",
    "df_Li = pd.read_csv('LiEtAl.csv', index_col=0)\n",
    "\n",
    "# Calculating means and 95% confidence intervals for all sub-space sizes examined\n",
    "mean = []\n",
    "HW = [] # using a t-test to calculate half-width\n",
    "parameters = []\n",
    "for i in range(len(df_Li.iloc[1:,0])):\n",
    "    parameters.append(df_Li.iloc[i+1,0])\n",
    "    mean.append(np.mean(df_Li.iloc[i+1,1:]))\n",
    "    \n",
    "    # Half-width\n",
    "    std = np.std(df_Li.iloc[i+1,1:], ddof=1)\n",
    "    n = len(df_Li.iloc[i+1,1:])\n",
    "    HW.append(stats.t.ppf(1 - (1 - 0.95) / 2, n-1) * (std / np.sqrt(n)))\n",
    "\n",
    "x = [min(df_Li.iloc[1:,0]),max(df_Li.iloc[1:,0])]\n",
    "y = [0.9*np.mean(df_Li.iloc[0,1:]),0.9*np.mean(df_Li.iloc[0,1:])]\n",
    "\n",
    "# The actual plotting\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(x, y, 'k--')\n",
    "plt.errorbar(parameters, mean, yerr=[HW, HW], fmt='o-', capsize=4, color=good_colors[0])\n",
    "plt.xlabel('Number of Tunable Parameters',size='18')\n",
    "plt.ylabel('Validation Accuracy',size='18')\n",
    "# plt.xscale('log')\n",
    "plt.legend(['90% of Full Network Accuracy'], fontsize='14')\n",
    "plt.xticks(fontsize='14')\n",
    "plt.yticks(fontsize='14')\n",
    "plt.grid()\n",
    "plt.savefig('Breast_cancer_Lietal.png', bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86caee2f-52a6-4056-a7bb-ed2616404ceb",
   "metadata": {},
   "source": [
    "# Frankle & Carbin Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39588ff-0987-453e-9599-bca768c38016",
   "metadata": {},
   "source": [
    "In the following we present the implementation of the Frankle and Carbin method used for neural network ID estimation as was described in section 5.2 based on the original work by Frankle and Carbin from 2019 (https://arxiv.org/abs/1803.03635). The first part deals with hyperparameter tuning - including the dimensions of a fully connected network architecture that would be used as the basis for the employment of the Frankle and Carbin method. The following part deals with the ID estimation itself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87db930-b0dc-4378-8394-bff6bf0929f9",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7891677-d897-400f-b655-8a5420eb2db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a class that assists with feeding data into PyTorch\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        feature = self.features[index]\n",
    "        label = self.labels[index]\n",
    "        return feature, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1035a940-5b53-4114-9638-97326e1095b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for training\n",
    "def train(dataloader, model, loss_fn, optimizer, device):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.train()\n",
    "    train_loss, correct = 0, 0\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y.view(-1, 1))\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc9cc96-a641-4e66-827f-b5f1840b0f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for testing\n",
    "def test(dataloader, model, loss_fn,device):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred, y.view(-1, 1))\n",
    "            pred = pred.round()\n",
    "            \n",
    "            correct += (pred == y.view(-1, 1)).sum().item()\n",
    "            test_loss += loss.item()\n",
    "    correct /= size\n",
    "    test_loss /= num_batches\n",
    "    return correct, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd14b29-cda2-4a94-a272-ae63b75b7e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A PyTorch based class for a two hidden layer fully connected architecture\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self,hid1,hid2):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(30, hid1)\n",
    "        self.fc2 = nn.Linear(hid1, hid2)\n",
    "        self.fc3 = nn.Linear(hid2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac7ceb9-005c-4b56-9ec9-0736f93c75d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for 200 epoch training and evaluation with an early stopping condition of minimum validation loss\n",
    "def train_and_evaluate(model, learning_rate, batch_size, train_dataset, val_dataset, device):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=len(val_dataset))\n",
    "    best_acc = - np.inf   # init to negative infinity\n",
    "    best_loss = + np.inf # init to positive infinity\n",
    "    loss_fn = nn.BCELoss()\n",
    "    for epoch in range(200):\n",
    "        train(train_loader, model, loss_fn, optimizer, device)\n",
    "        test_acc, loss = test(val_loader, model, loss_fn, device)\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            best_acc = test_acc\n",
    "    print('Accuracy: ' + str(100*test_acc) + '%')\n",
    "    return best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b9c284-aaa2-48cf-a85a-bbbe0674d051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read data into objects that PyTorch can handle - it is required to be in function form for\n",
    "# compatibility with the optuna library\n",
    "def load_data():\n",
    "    # Reading\n",
    "    df3 = pd.read_csv('Breast_cancer.csv')\n",
    "    df4 = df3.drop(['id','Unnamed: 32'], axis = 1)\n",
    "    columns = list(df4.columns)\n",
    "    columns.remove('diagnosis')\n",
    "\n",
    "    # Encoding the labels\n",
    "    class_to_idx = {class_label: idx for idx, class_label in enumerate(df4['diagnosis'].unique())}\n",
    "    df4['label_idx'] = df4['diagnosis'].map(class_to_idx)\n",
    "\n",
    "    # Splitting test and validation\n",
    "    X_train, X_val, y_train, y_val = train_test_split(df4[columns],df4['label_idx'],test_size=0.3,random_state=42)\n",
    "\n",
    "    # Normalizing\n",
    "    for column in columns:\n",
    "        mean = X_train[column].mean()\n",
    "        std = X_train[column].std()\n",
    "        X_train[column] = (X_train[column]-mean)/std\n",
    "        X_val[column] = (X_val[column]-mean)/std\n",
    "\n",
    "    # Transforming data to a Pytorch form\n",
    "    X_train = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "    X_val = torch.tensor(X_val.values, dtype=torch.float32)\n",
    "    y_train = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "    y_val = torch.tensor(y_val.values, dtype=torch.float32)\n",
    "\n",
    "    train_dataset = CustomDataset(X_train, y_train)\n",
    "    val_dataset = CustomDataset(X_val, y_val)\n",
    "    return train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53973a7-34f2-4f89-99d5-9c2e99845e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the objective function used by optuna to optimize the hyperparameters\n",
    "def objective(trial):\n",
    "    train_data, val_data = load_data()\n",
    "    \n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)\n",
    "    batch_size = trial.suggest_int('batch_size', 4, 64, log=True)\n",
    "    hid1 = trial.suggest_int('hid1', 10, 100, log=True)\n",
    "    hid2 = trial.suggest_int('hid2', 10, 100, log=True)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = MyModel(hid1,hid2).to(device)\n",
    "    loss = train_and_evaluate(model, learning_rate, batch_size, train_data, val_data, device)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4e30a7-7f84-49b9-9983-d4bbe167b662",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Performing optimization using optuna\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "best_params = study.best_params\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9dd1f3-336e-42bd-86bd-ee6bdb48f774",
   "metadata": {},
   "source": [
    "## ID Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91570fec-d57a-4bba-b63a-1320c542fac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a class that assists with feeding data into PyTorch\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        feature = self.features[index]\n",
    "        label = self.labels[index]\n",
    "        return feature, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeeff9f9-ffbf-42d0-8809-38fcc5eba91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data prep\n",
    "\n",
    "# Reading\n",
    "df3 = pd.read_csv('Breast_cancer.csv')\n",
    "df4 = df3.drop(['id','Unnamed: 32'], axis = 1)\n",
    "columns = list(df4.columns)\n",
    "columns.remove('diagnosis')\n",
    "\n",
    "# Encoding the labels\n",
    "class_to_idx = {class_label: idx for idx, class_label in enumerate(df4['diagnosis'].unique())}\n",
    "df4['label_idx'] = df4['diagnosis'].map(class_to_idx)\n",
    "\n",
    "# Splitting test and validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(df4[columns],df4['label_idx'],test_size=0.3,random_state=42)\n",
    "\n",
    "# Normalizing\n",
    "for column in columns:\n",
    "    mean = X_train[column].mean()\n",
    "    std = X_train[column].std()\n",
    "    X_train[column] = (X_train[column]-mean)/std\n",
    "    X_val[column] = (X_val[column]-mean)/std\n",
    "\n",
    "# Transforming data to a Pytorch form\n",
    "X_train = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "X_val = torch.tensor(X_val.values, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val.values, dtype=torch.float32)\n",
    "\n",
    "batch_size = 8\n",
    "train_dataset = CustomDataset(X_train, y_train)\n",
    "val_dataset = CustomDataset(X_val, y_val)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4228a9a6-3b79-4329-965c-7673175ff5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for training\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y.view(-1, 1))\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d0fef0-b9df-4753-a60d-a1760a038137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for testing\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred, y.view(-1, 1))\n",
    "            pred = pred.round()\n",
    "            \n",
    "            correct += (pred == y.view(-1, 1)).sum().item()\n",
    "            test_loss += loss.item()\n",
    "    correct /= size\n",
    "    test_loss /= num_batches\n",
    "    return correct, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2126635b-bde7-4d93-a465-9fd1f7e5bf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for 200 epoch training and evaluation with an early stopping condition of minimum validation loss\n",
    "def train_and_evaluate(model, learning_rate, train_loader, val_loader):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    best_acc = - np.inf   # init to negative infinity\n",
    "    best_loss = + np.inf # init to positive infinity\n",
    "    loss_fn = nn.BCELoss()\n",
    "    for epoch in range(200):\n",
    "        train(train_loader, model, loss_fn, optimizer)\n",
    "        test_acc, loss = test(val_loader, model, loss_fn)\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            best_acc = test_acc\n",
    "    return best_acc, best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd71ac07-a712-4b90-8674-e9b23b7fcec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A PyTorch based class for a two hidden layer fully connected architecture\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(30, 51)\n",
    "        self.fc2 = nn.Linear(51, 39)\n",
    "        self.fc3 = nn.Linear(39, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fac054-c076-415e-8017-f93a38df689c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that verifies the amount of weights pruned and helps to keep track on the pruning percentage\n",
    "def count_zeros(model):\n",
    "    return torch.sum(model.fc1.weight.data == 0.0).item() + torch.sum(model.fc1.bias.data == 0.0).item() + torch.sum(model.fc2.weight.data == 0.0).item() + torch.sum(model.fc2.bias.data == 0.0).item() + torch.sum(model.fc3.weight.data == 0.0).item() + torch.sum(model.fc3.bias.data == 0.0).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194fcbb3-199d-4b89-be30-60d69826b835",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generating results\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(torch.cuda.get_device_name(device))\n",
    "all_weights = 31*51 + 52*39 + 40 # For calculation of pruning percentage\n",
    "remaining = []\n",
    "val_accuracy = []\n",
    "prunings = 52 # 25 - moderate, 52 - slow, 11 - fast (how many pruning iterations should be performed)\n",
    "\n",
    "learning_rate = 0.0037994898049290024\n",
    "replicas = 30 # We use 30 replicas to have statistics of the results\n",
    "remaining = np.zeros([replicas,prunings+1])\n",
    "val_accuracy = np.zeros([replicas,prunings+1])\n",
    "\n",
    "for rep in range(replicas):\n",
    "    model = MyModel().to(device)\n",
    "    initial_state_dict = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    for i in range(prunings+1):\n",
    "        acc, loss = train_and_evaluate(model, learning_rate, train_loader, val_loader)\n",
    "        remaining[rep][i] = 100*(all_weights - count_zeros(model))/all_weights\n",
    "        val_accuracy[rep][i] = acc\n",
    "        print('Replica: ' + str(rep+1) + '. Remaining weights are ' + str(remaining[rep][i]) + '%. Validation accuracy is ' + str(acc*100) + '%.')\n",
    "        \n",
    "        # Apply pruning to the specified layers, changing the pruning rates by commenting and uncommenting lines\n",
    "        to_prune = [(model.fc1, 'weight', 0.1), (model.fc2, 'weight', 0.1), (model.fc3, 'weight', 0.05)] # slow\n",
    "        # to_prune = [(model.fc1, 'weight', 0.2), (model.fc2, 'weight', 0.2), (model.fc3, 'weight', 0.1)] # moderate\n",
    "        # to_prune = [(model.fc1, 'weight', 0.4), (model.fc2, 'weight', 0.4), (model.fc3, 'weight', 0.2)] # fast\n",
    "        for layer, param_name, pruning_perc in to_prune:\n",
    "            prune.l1_unstructured(layer, name=param_name, amount=pruning_perc)\n",
    "        \n",
    "        # Upload original parameter values\n",
    "        model.fc1.weight.data = initial_state_dict['fc1.weight']\n",
    "        model.fc1.bias.data = initial_state_dict['fc1.bias']\n",
    "        model.fc2.weight.data = initial_state_dict['fc2.weight']\n",
    "        model.fc2.bias.data = initial_state_dict['fc2.bias']\n",
    "        model.fc3.weight.data = initial_state_dict['fc3.weight']\n",
    "        model.fc3.bias.data = initial_state_dict['fc3.bias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43407357-066b-4dad-9624-0b9de934f5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results (change name based on pruning rate used)\n",
    "df_fast = pd.DataFrame(val_accuracy)\n",
    "df_fast.loc[len(df_fast.index)] = remaining[0]\n",
    "df_fast.to_csv(\"Fast.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be328b53-b465-4459-b853-1d95d19daa82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# One shot pruning\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(torch.cuda.get_device_name(device))\n",
    "all_weights = 31*51 + 52*39 + 40 # For calculation of pruning percentage\n",
    "remaining = []\n",
    "val_accuracy = []\n",
    "prunings = 11 # Number of pruning for One Shot always matched the one used for fast rate pruning\n",
    "\n",
    "learning_rate = 0.0037994898049290024\n",
    "replicas = 30 # We use 30 replicas to have statistics of the results\n",
    "remaining = np.zeros([replicas,prunings+1])\n",
    "val_accuracy = np.zeros([replicas,prunings+1])\n",
    "\n",
    "for rep in range(replicas):\n",
    "    model = MyModel().to(device)\n",
    "    initial_state_dict = copy.deepcopy(model.state_dict())\n",
    "    \n",
    "    # Before pruining\n",
    "    model_temp = MyModel().to(device)\n",
    "    model_temp.load_state_dict(initial_state_dict)\n",
    "    acc, loss = train_and_evaluate(model_temp, learning_rate, train_loader, val_loader, device)\n",
    "    remaining[rep][0] = 100*(all_weights - count_zeros(model_temp))/all_weights # Should be 100, this is a sanity check\n",
    "    val_accuracy[rep][0] = acc\n",
    "    print('Replica: ' + str(rep+1) + '. Remaining weights are ' + str(remaining[rep][0]) + '%. Validation accuracy is ' + str(acc*100) + '%.')\n",
    "    \n",
    "    # Including prunings\n",
    "    for i in range(prunings):\n",
    "        # Training a new model\n",
    "        model_temp = MyModel().to(device)\n",
    "        model_temp.load_state_dict(initial_state_dict)\n",
    "        acc, loss = train_and_evaluate(model_temp, learning_rate, train_loader, val_loader, device)\n",
    "        \n",
    "        # One Shot pruning\n",
    "        mid_layer_prune = 1 - 0.6**(i+1)\n",
    "        out_layer_prune = 1 - 0.8**(i+1)\n",
    "        to_prune = [(model_temp.fc1, 'weight', mid_layer_prune), (model_temp.fc2, 'weight', mid_layer_prune), (model_temp.fc3, 'weight', out_layer_prune)]\n",
    "        for layer, param_name, pruning_perc in to_prune:\n",
    "            prune.l1_unstructured(layer, name=param_name, amount=pruning_perc)\n",
    "            \n",
    "        # Upload original parameter values\n",
    "        model_temp.fc1.weight.data = initial_state_dict['fc1.weight']\n",
    "        model_temp.fc1.bias.data = initial_state_dict['fc1.bias']\n",
    "        model_temp.fc2.weight.data = initial_state_dict['fc2.weight']\n",
    "        model_temp.fc2.bias.data = initial_state_dict['fc2.bias']\n",
    "        model_temp.fc3.weight.data = initial_state_dict['fc3.weight']\n",
    "        model_temp.fc3.bias.data = initial_state_dict['fc3.bias']\n",
    "        \n",
    "        # Training the post-pruning model\n",
    "        acc, loss = train_and_evaluate(model_temp, learning_rate, train_loader, val_loader, device)\n",
    "        remaining[rep][i+1] = 100*(all_weights - count_zeros(model_temp))/all_weights\n",
    "        val_accuracy[rep][i+1] = acc\n",
    "        print('Replica: ' + str(rep+1) + '. Remaining weights are ' + str(remaining[rep][i+1]) + '%. Validation accuracy is ' + str(acc*100) + '%.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80c9f33-4747-4acb-aada-8b6142a833f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "df_one = pd.DataFrame(val_accuracy)\n",
    "df_one.loc[len(df_one.index)] = remaining[0]\n",
    "df_one.to_csv(\"OneShot.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667c7014-0536-41f1-9477-2ce2ea5de54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function used for plotting the x ticks as desired\n",
    "def format_tick(x, pos):\n",
    "    return \"{:.2f}\".format(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a43317-fbe4-4616-8f9b-d4ba3bdd104e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "\n",
    "# Reading data\n",
    "df_one = pd.read_csv('OneShot.csv', index_col=0)\n",
    "df_slow = pd.read_csv('Slow.csv', index_col=0)\n",
    "df_mod = pd.read_csv('Moderate.csv', index_col=0)\n",
    "df_fast = pd.read_csv('Fast.csv', index_col=0)\n",
    "\n",
    "all_weights = 31*51 + 52*39 + 40\n",
    "\n",
    "# Calculating confidence intervals for all pruning options\n",
    "weights_one = []\n",
    "mean_one = []\n",
    "std_one = []\n",
    "for i in range(len(df_one.columns)):\n",
    "    mean_one.append(np.mean(df_one.iloc[:30,i]))\n",
    "    std_one.append(np.std(df_one.iloc[:30,i]))\n",
    "    weights_one.append(df_one.iloc[df_one.index[-1],i])\n",
    "\n",
    "LB_one = []\n",
    "for i in range(len(df_one.columns)):\n",
    "    # Half-width\n",
    "    std = np.std(df_one.iloc[:30,i], ddof=1)\n",
    "    n = len(df_one.iloc[:30,i])\n",
    "    HW = stats.t.ppf(1 - (1 - 0.95) / 2, n-1) * (std / np.sqrt(n))\n",
    "    LB_one.append(mean_one[i]-HW)\n",
    "    \n",
    "weights_slow = []\n",
    "mean_slow = []\n",
    "std_slow = []\n",
    "for i in range(len(df_slow.columns)):\n",
    "    mean_slow.append(np.mean(df_slow.iloc[:30,i]))\n",
    "    std_slow.append(np.std(df_slow.iloc[:30,i]))\n",
    "    weights_slow.append(df_slow.iloc[df_slow.index[-1],i])\n",
    "\n",
    "LB_slow = []\n",
    "for i in range(len(df_slow.columns)):\n",
    "    # Half-width\n",
    "    std = np.std(df_slow.iloc[:30,i], ddof=1)\n",
    "    n = len(df_slow.iloc[:30,i])\n",
    "    HW = stats.t.ppf(1 - (1 - 0.95) / 2, n-1) * (std / np.sqrt(n))\n",
    "    LB_slow.append(mean_slow[i]-HW)\n",
    "    \n",
    "weights_mod = []\n",
    "mean_mod = []\n",
    "std_mod = []\n",
    "for i in range(len(df_mod.columns)):\n",
    "    mean_mod.append(np.mean(df_mod.iloc[:30,i]))\n",
    "    std_mod.append(np.std(df_mod.iloc[:30,i]))\n",
    "    weights_mod.append(df_mod.iloc[df_mod.index[-1],i])\n",
    "\n",
    "LB_mod = []\n",
    "for i in range(len(df_mod.columns)):\n",
    "    # Half-width\n",
    "    std = np.std(df_mod.iloc[:30,i], ddof=1)\n",
    "    n = len(df_mod.iloc[:30,i])\n",
    "    HW = stats.t.ppf(1 - (1 - 0.95) / 2, n-1) * (std / np.sqrt(n))\n",
    "    LB_mod.append(mean_mod[i]-HW)\n",
    "\n",
    "weights_fast = []\n",
    "mean_fast = []\n",
    "std_fast = []\n",
    "for i in range(len(df_fast.columns)):\n",
    "    mean_fast.append(np.mean(df_fast.iloc[:30,i]))\n",
    "    std_fast.append(np.std(df_fast.iloc[:30,i]))\n",
    "    weights_fast.append(df_fast.iloc[df_fast.index[-1],i])\n",
    "\n",
    "LB_fast = []\n",
    "for i in range(len(df_fast.columns)):\n",
    "    # Half-width\n",
    "    std = np.std(df_fast.iloc[:30,i], ddof=1)\n",
    "    n = len(df_fast.iloc[:30,i])\n",
    "    HW = stats.t.ppf(1 - (1 - 0.95) / 2, n-1) * (std / np.sqrt(n))\n",
    "    LB_fast.append(mean_fast[i]-HW)\n",
    "\n",
    "# The actual plotting\n",
    "TH = 0.9*mean_slow[0]\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.errorbar(weights_slow, mean_slow, yerr=[std_slow, std_slow], fmt='o-', capsize=4, color=good_colors[0])\n",
    "plt.errorbar(weights_mod, mean_mod, yerr=[std_mod, std_mod], fmt='o-', capsize=4, color=good_colors[1])\n",
    "plt.errorbar(weights_fast, mean_fast, yerr=[std_fast, std_fast], fmt='o-', capsize=4, color=good_colors[2])\n",
    "plt.errorbar(weights_one, mean_one, yerr=[std_one, std_one], fmt='o-', capsize=4, color=good_colors[3])\n",
    "plt.plot([min(min(weights_slow),min(weights_mod),min(weights_fast),min(weights_one)),100],[TH,TH],'k--')\n",
    "plt.xlabel('Remaining Weights [%]',size='18')\n",
    "plt.ylabel('Validation Accuracy',size='18')\n",
    "plt.xscale('log')\n",
    "plt.legend(['90% of Full Network Accuracy','Iterative Pruining - Slow Rate', 'Iterative Pruining - Moderate Rate', 'Iterative Pruining - Fast Rate', 'One Shot Pruining'], fontsize='14')\n",
    "ax = plt.gca()\n",
    "ax.xaxis.set_major_formatter(ticker.FuncFormatter(format_tick))\n",
    "ax.invert_xaxis()\n",
    "plt.xticks(fontsize='14')\n",
    "plt.yticks(fontsize='14')\n",
    "plt.grid()\n",
    "plt.savefig('LTH_no_zoom.png', bbox_inches=\"tight\")\n",
    "plt.show()    \n",
    "\n",
    "# Printing the ID estimates by comparing to the 90% threshold\n",
    "for i in range(len(weights_one)):\n",
    "    if LB_one[i] < TH:\n",
    "        print('For One shot ID is at ' + str(weights_one[i]) + '-' + str(weights_one[i-1]) + ' with ' + str(weights_one[i]*all_weights/100) + '-' + str(weights_one[i-1]*all_weights/100) + ' parameters')\n",
    "        break\n",
    "for i in range(len(weights_fast)):\n",
    "    if LB_fast[i] < TH:\n",
    "        print('For fast ID is at ' + str(weights_fast[i]) + '-' + str(weights_fast[i-1]) + ' with ' + str(weights_fast[i]*all_weights/100) + '-' + str(weights_fast[i-1]*all_weights/100) + ' parameters')\n",
    "        break\n",
    "for i in range(len(weights_mod)):\n",
    "    if LB_mod[i] < TH:\n",
    "        print('For mod ID is at ' + str(weights_mod[i]) + '-' + str(weights_mod[i-1]) + ' with ' + str(weights_mod[i]*all_weights/100) + '-' + str(weights_mod[i-1]*all_weights/100) + ' parameters')\n",
    "        break\n",
    "for i in range(len(weights_slow)):\n",
    "    if LB_slow[i] < TH:\n",
    "        print('For slow ID is at ' + str(weights_slow[i]) + '-' + str(weights_slow[i-1]) + ' with ' + str(weights_slow[i]*all_weights/100) + '-' + str(weights_slow[i-1]*all_weights/100) + ' parameters')\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80988b66-84b6-47cc-ab7e-f01e878be606",
   "metadata": {},
   "source": [
    "# Comparison to Fully Connected models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde0c27d-b3ac-492c-be9d-aca464bc208d",
   "metadata": {},
   "source": [
    "In the following we simple fully connected models to compare to the resulting models from emplying the Li et al. and Frankle and Carbin methods, as described in section 5.1. Per usual, the first part presents the hyperparameter tuning and the following part the results acquisition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb02a13-d569-4100-b121-0111fb4d800b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the dimensions of the fully connected model\n",
    "hid1 = 4\n",
    "hid2 = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0114d6d7-a8eb-4bf6-b4bf-811ba827cd52",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd13bb8-e5ef-48b3-b540-1cb21cb5724a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a class that assists with feeding data into PyTorch\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        feature = self.features[index]\n",
    "        label = self.labels[index]\n",
    "        return feature, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dffa8bd-0874-4ccf-b632-66c02eee4469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for training\n",
    "def train(dataloader, model, loss_fn, optimizer, device):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.train()\n",
    "    train_loss, correct = 0, 0\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y.view(-1, 1))\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b99b79f-9103-4166-aba4-9de0809ba4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for testing\n",
    "def test(dataloader, model, loss_fn,device):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred, y.view(-1, 1))\n",
    "            pred = pred.round()\n",
    "            \n",
    "            correct += (pred == y.view(-1, 1)).sum().item()\n",
    "            test_loss += loss.item()\n",
    "    correct /= size\n",
    "    test_loss /= num_batches\n",
    "    return correct, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea3ef18-a869-46d5-b6a3-c14fb51178bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for 200 epoch training and evaluation with an early stopping condition of minimum validation loss\n",
    "def train_and_evaluate(model, learning_rate, batch_size, train_dataset, val_dataset, device):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=len(val_dataset))\n",
    "    best_acc = - np.inf   # init to negative infinity\n",
    "    best_loss = + np.inf # init to positive infinity\n",
    "    loss_fn = nn.BCELoss()\n",
    "    for epoch in range(200):\n",
    "        train(train_loader, model, loss_fn, optimizer, device)\n",
    "        test_acc, loss = test(val_loader, model, loss_fn, device)\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            best_acc = test_acc\n",
    "    print('Accuracy: ' + str(100*test_acc) + '%')\n",
    "    return best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed9e7a2-6712-4900-bc54-2972a34b6ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read data into objects that PyTorch can handle - it is required to be in function form for\n",
    "# compatibility with the optuna library\n",
    "def load_data():\n",
    "    # Reading\n",
    "    df3 = pd.read_csv('Breast_cancer.csv')\n",
    "    df4 = df3.drop(['id','Unnamed: 32'], axis = 1)\n",
    "    columns = list(df4.columns)\n",
    "    columns.remove('diagnosis')\n",
    "\n",
    "    # Encoding the labels\n",
    "    class_to_idx = {class_label: idx for idx, class_label in enumerate(df4['diagnosis'].unique())}\n",
    "    df4['label_idx'] = df4['diagnosis'].map(class_to_idx)\n",
    "\n",
    "    # Splitting test and validation\n",
    "    X_train, X_val, y_train, y_val = train_test_split(df4[columns],df4['label_idx'],test_size=0.3,random_state=42)\n",
    "\n",
    "    # Normalizing\n",
    "    for column in columns:\n",
    "        mean = X_train[column].mean()\n",
    "        std = X_train[column].std()\n",
    "        X_train[column] = (X_train[column]-mean)/std\n",
    "        X_val[column] = (X_val[column]-mean)/std\n",
    "\n",
    "    # Transforming data to a Pytorch form\n",
    "    X_train = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "    X_val = torch.tensor(X_val.values, dtype=torch.float32)\n",
    "    y_train = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "    y_val = torch.tensor(y_val.values, dtype=torch.float32)\n",
    "\n",
    "    train_dataset = CustomDataset(X_train, y_train)\n",
    "    val_dataset = CustomDataset(X_val, y_val)\n",
    "    return train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ab83cf-215a-4456-a29d-94310657c75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A PyTorch based class for a two hidden layer fully connected architecture\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(30, hid1)\n",
    "        self.fc2 = nn.Linear(hid1, hid2)\n",
    "        self.fc3 = nn.Linear(hid2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da8bd78-eaac-453e-832f-44160ff29829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the objective function used by optuna to optimize the hyperparameters\n",
    "def objective(trial):\n",
    "    train_data, val_data = load_data()\n",
    "    \n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n",
    "    batch_size = trial.suggest_int('batch_size', 4, 64, log=True)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = MyModel().to(device)\n",
    "    loss = train_and_evaluate(model, learning_rate, batch_size, train_data, val_data, device)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9757f20e-cbb0-42de-aaee-8ae3fe3b9bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing optimization using optuna\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "best_params = study.best_params\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27488db-b248-4d8f-b549-50c170a3ecc4",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d982a5-6f15-44da-9dc3-2a7abf7cf2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a class that assists with feeding data into PyTorch\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        feature = self.features[index]\n",
    "        label = self.labels[index]\n",
    "        return feature, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99338e4-7763-4710-b01f-0faa6bc2be96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data prep\n",
    "\n",
    "# Reading\n",
    "df3 = pd.read_csv('Breast_cancer.csv')\n",
    "df4 = df3.drop(['id','Unnamed: 32'], axis = 1)\n",
    "columns = list(df4.columns)\n",
    "columns.remove('diagnosis')\n",
    "\n",
    "# Encoding the labels\n",
    "class_to_idx = {class_label: idx for idx, class_label in enumerate(df4['diagnosis'].unique())}\n",
    "df4['label_idx'] = df4['diagnosis'].map(class_to_idx)\n",
    "\n",
    "# Splitting test and validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(df4[columns],df4['label_idx'],test_size=0.3,random_state=42)\n",
    "\n",
    "# Normalizing\n",
    "for column in columns:\n",
    "    mean = X_train[column].mean()\n",
    "    std = X_train[column].std()\n",
    "    X_train[column] = (X_train[column]-mean)/std\n",
    "    X_val[column] = (X_val[column]-mean)/std\n",
    "\n",
    "# Transforming data to a Pytorch form\n",
    "X_train = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "X_val = torch.tensor(X_val.values, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val.values, dtype=torch.float32)\n",
    "\n",
    "batch_size = 7 \n",
    "train_dataset = CustomDataset(X_train, y_train)\n",
    "val_dataset = CustomDataset(X_val, y_val)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4875d85-3f49-43fb-9002-3b7016c8711f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for training\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y.view(-1, 1))\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912fba92-622a-4e03-b63f-6f08306a83fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for testing\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred, y.view(-1, 1))\n",
    "            pred = pred.round()\n",
    "            \n",
    "            correct += (pred == y.view(-1, 1)).sum().item()\n",
    "            test_loss += loss.item()\n",
    "    correct /= size\n",
    "    test_loss /= num_batches\n",
    "    return correct, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af95392-bf91-4b4a-9cd2-5517ee47c8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for 200 epoch training and evaluation with an early stopping condition of minimum validation loss\n",
    "def train_and_evaluate(model, learning_rate, train_loader, val_loader):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    best_acc = - np.inf   # init to negative infinity\n",
    "    best_loss = + np.inf # init to positive infinity\n",
    "    loss_fn = nn.BCELoss()\n",
    "    for epoch in range(200):\n",
    "        train(train_loader, model, loss_fn, optimizer)\n",
    "        test_acc, loss = test(val_loader, model, loss_fn)\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            best_acc = test_acc\n",
    "    return best_acc, best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481a50e5-8d46-4dcf-93db-6d5ec3b6d9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A PyTorch based class for a two hidden layer fully connected architecture\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(30, hid1)\n",
    "        self.fc2 = nn.Linear(hid1, hid2)\n",
    "        self.fc3 = nn.Linear(hid2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d95a9fc-09ac-429e-892a-66dff938657d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating results\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(torch.cuda.get_device_name(device))\n",
    "\n",
    "learning_rate = 0.005379133453785\n",
    "replicas = 30 # We use 30 replicas to have statistics of the results\n",
    "res = []\n",
    "\n",
    "for rep in range(replicas):\n",
    "    model = MyModel().to(device)\n",
    "    acc, loss = train_and_evaluate(model, learning_rate, train_loader, val_loader)\n",
    "    res.append(acc)\n",
    "    print('Replica: ' + str(rep+1) + '. Validation accuracy is ' + str(acc*100) + '%.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43218c49-7d89-4be5-9881-97a7a1c52a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the results and printing the 95% confidence interval of the accuracy\n",
    "df = pd.DataFrame(res)\n",
    "df.to_csv(\"FC_143_weights.csv\")\n",
    "print(np.mean(res))\n",
    "std = np.std(res, ddof=1)\n",
    "n = len(res)\n",
    "print(stats.t.ppf(1 - (1 - 0.95) / 2, n-1) * (std / np.sqrt(n)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
